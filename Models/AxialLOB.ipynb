{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ddc7ba-ef39-4bbe-b69e-f9e413bdb4bd",
   "metadata": {},
   "source": [
    "#### AxialLOB: Convolutional Layers, Gated Axial Attention Modules, Residual Connections, Pooling, and Fully Connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abde1957-7d3d-46ef-9400-e840aafced2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9963d786-1eab-4f72-a5c4-141e84f94418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef1f4e7-f25f-4687-8200-6fb713fa356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def prepare_x(data):\n",
    "    df1 = data[:40, :].T\n",
    "    return np.array(df1)\n",
    "\n",
    "def get_label(data):\n",
    "    lob = data[-5:, :].T\n",
    "    return lob\n",
    "\n",
    "def data_classification(X, Y, T):\n",
    "    [N, D] = X.shape\n",
    "    df = np.array(X)\n",
    "\n",
    "    dY = np.array(Y)\n",
    "\n",
    "    dataY = dY[T - 1:N]\n",
    "\n",
    "    dataX = np.zeros((N - T + 1, T, D))\n",
    "    for i in range(T, N + 1):\n",
    "        dataX[i - T] = df[i - T:i, :]\n",
    "\n",
    "    return dataX, dataY\n",
    "\n",
    "def torch_data(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    x = torch.unsqueeze(x, 1)\n",
    "    y = torch.from_numpy(y)\n",
    "    y = F.one_hot(y, num_classes=3)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e30cee-ac97-4256-866f-93b95005d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class Dataset(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, data, k, num_classes, T):\n",
    "        \"\"\"Initialization\"\"\" \n",
    "        self.k = k\n",
    "        self.num_classes = num_classes\n",
    "        self.T = T\n",
    "            \n",
    "        x = prepare_x(data)\n",
    "        y = get_label(data)\n",
    "        x, y = data_classification(x, y, self.T)\n",
    "        y = y[:,self.k] - 1\n",
    "        self.length = len(x)\n",
    "\n",
    "        x = torch.from_numpy(x)\n",
    "        self.x = torch.unsqueeze(x, 1)\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates samples of data\"\"\"\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ef244d-d390-4142-990b-c4e89bdfd89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.zip already exists.\n",
      "Extracting data...\n",
      "Data extraction completed.\n"
     ]
    }
   ],
   "source": [
    "def download_and_extract_data(data_url, data_zip_path, data_folder):\n",
    "    if not os.path.isfile(data_zip_path):\n",
    "        print('Downloading data...')\n",
    "        response = requests.get(data_url, stream=True)\n",
    "        with open(data_zip_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print('Data download completed.')\n",
    "\n",
    "        # Extract data.zip\n",
    "        print('Extracting data...')\n",
    "        with zipfile.ZipFile(data_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        print('Data extraction completed.')\n",
    "    else:\n",
    "        print('data.zip already exists.')\n",
    "        # Check if the extracted folder exists\n",
    "        if not os.path.exists(data_folder):\n",
    "            print('Extracting data...')\n",
    "            with zipfile.ZipFile(data_zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall('.')\n",
    "            print('Data extraction completed.')\n",
    "        else:\n",
    "            print('Data already extracted.')\n",
    "\n",
    "# Set data URL and paths\n",
    "data_url = 'https://raw.githubusercontent.com/zcakhaa/DeepLOB-Deep-Convolutional-Neural-Networks-for-Limit-Order-Books/master/data/data.zip'\n",
    "data_zip_path = 'data.zip'\n",
    "data_folder = 'data'  # Extracted folder name\n",
    "\n",
    "# Download and extract data\n",
    "download_and_extract_data(data_url, data_zip_path, data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95849c8d-e92a-4424-81ec-e194055982ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 203800) (149, 50950) (149, 139587)\n"
     ]
    }
   ],
   "source": [
    "# Load training and validation data\n",
    "dec_data = np.loadtxt('Train_Dst_NoAuction_DecPre_CF_7.txt')\n",
    "dec_train = dec_data[:, :int(np.floor(dec_data.shape[1] * 0.8))]\n",
    "dec_val = dec_data[:, int(np.floor(dec_data.shape[1] * 0.8)):]\n",
    "\n",
    "# Load test data\n",
    "dec_test1 = np.loadtxt('Test_Dst_NoAuction_DecPre_CF_7.txt')\n",
    "dec_test2 = np.loadtxt('Test_Dst_NoAuction_DecPre_CF_8.txt')\n",
    "dec_test3 = np.loadtxt('Test_Dst_NoAuction_DecPre_CF_9.txt')\n",
    "dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "\n",
    "# Print data shapes\n",
    "print(dec_train.shape, dec_val.shape, dec_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e47585b-b606-452b-b334-4e1d6638f170",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.07 GiB for an array with shape (203701, 100, 40) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Creating Dataset Instances and Data Loaders\u001b[39;00m\n\u001b[0;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m----> 4\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m Dataset(data\u001b[38;5;241m=\u001b[39mdec_train, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, T\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      5\u001b[0m dataset_val \u001b[38;5;241m=\u001b[39m Dataset(data\u001b[38;5;241m=\u001b[39mdec_val, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, T\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      6\u001b[0m dataset_test \u001b[38;5;241m=\u001b[39m Dataset(data\u001b[38;5;241m=\u001b[39mdec_test, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, T\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[1;34m(self, data, k, num_classes, T)\u001b[0m\n\u001b[0;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m prepare_x(data)\n\u001b[0;32m     11\u001b[0m y \u001b[38;5;241m=\u001b[39m get_label(data)\n\u001b[1;32m---> 12\u001b[0m x, y \u001b[38;5;241m=\u001b[39m data_classification(x, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     13\u001b[0m y \u001b[38;5;241m=\u001b[39m y[:,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x)\n",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m, in \u001b[0;36mdata_classification\u001b[1;34m(X, Y, T)\u001b[0m\n\u001b[0;32m     14\u001b[0m dY \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Y)\n\u001b[0;32m     16\u001b[0m dataY \u001b[38;5;241m=\u001b[39m dY[T \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:N]\n\u001b[1;32m---> 18\u001b[0m dataX \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((N \u001b[38;5;241m-\u001b[39m T \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, T, D))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T, N \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     20\u001b[0m     dataX[i \u001b[38;5;241m-\u001b[39m T] \u001b[38;5;241m=\u001b[39m df[i \u001b[38;5;241m-\u001b[39m T:i, :]\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 6.07 GiB for an array with shape (203701, 100, 40) and data type float64"
     ]
    }
   ],
   "source": [
    "# Creating Dataset Instances and Data Loaders\n",
    "\n",
    "batch_size = 64\n",
    "dataset_train = Dataset(data=dec_train, k=4, num_classes=3, T=100)\n",
    "dataset_val = Dataset(data=dec_val, k=4, num_classes=3, T=100)\n",
    "dataset_test = Dataset(data=dec_test, k=4, num_classes=3, T=100)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(dataset_train.x.shape, dataset_train.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5037fca-a337-426c-9561-c50fd233cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing dataset_train\n",
    "tmp_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=1, shuffle=True)\n",
    "\n",
    "for x, y in tmp_loader:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbc6f6a3-e06a-4b2b-8164-9f518b1d722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _conv1d1x1(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "        nn.BatchNorm1d(out_channels)\n",
    "    )\n",
    "\n",
    "class GatedAxialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads, dim, flag):\n",
    "        assert (in_channels % heads == 0) and (out_channels % heads == 0)\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.dim_head_v = out_channels // heads\n",
    "        self.flag = flag  # if flag then we do the attention along width\n",
    "        self.dim = dim\n",
    "        self.dim_head_qk = self.dim_head_v // 2\n",
    "        self.qkv_channels = self.dim_head_v + self.dim_head_qk * 2\n",
    "\n",
    "        # Multi-head self attention\n",
    "        self.to_qkv = _conv1d1x1(in_channels, self.heads * self.qkv_channels)\n",
    "        self.bn_qkv = nn.BatchNorm1d(self.heads * self.qkv_channels)\n",
    "        self.bn_similarity = nn.BatchNorm2d(heads * 3)\n",
    "        self.bn_output = nn.BatchNorm1d(self.heads * self.qkv_channels)\n",
    "\n",
    "        # Gating mechanism\n",
    "        self.f_qr = nn.Parameter(torch.tensor(0.3), requires_grad=False)\n",
    "        self.f_kr = nn.Parameter(torch.tensor(0.3), requires_grad=False)\n",
    "        self.f_sve = nn.Parameter(torch.tensor(0.3), requires_grad=False)\n",
    "        self.f_sv = nn.Parameter(torch.tensor(0.5), requires_grad=False)\n",
    "\n",
    "        # Position embedding\n",
    "        self.relative = nn.Parameter(torch.randn(self.dim_head_v * 2, dim * 2 - 1), requires_grad=True)\n",
    "        query_index = torch.arange(dim).unsqueeze(0)\n",
    "        key_index = torch.arange(dim).unsqueeze(1)\n",
    "        relative_index = key_index - query_index + dim - 1\n",
    "        self.register_buffer('flatten_index', relative_index.view(-1))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.flag:\n",
    "            x = x.permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            x = x.permute(0, 3, 1, 2)  # N, W, C, H\n",
    "        N, W, C, H = x.shape\n",
    "        x = x.contiguous().view(N * W, C, H)\n",
    "\n",
    "        # Transformations\n",
    "        x = self.to_qkv(x)\n",
    "        qkv = self.bn_qkv(x)\n",
    "        q, k, v = torch.split(\n",
    "            qkv.reshape(N * W, self.heads, self.dim_head_v * 2, H),\n",
    "            [self.dim_head_v // 2, self.dim_head_v // 2, self.dim_head_v],\n",
    "            dim=2\n",
    "        )\n",
    "\n",
    "        # Calculate position embedding\n",
    "        all_embeddings = torch.index_select(self.relative, 1, self.flatten_index).view(self.dim_head_v * 2, self.dim, self.dim)\n",
    "        q_embedding, k_embedding, v_embedding = torch.split(all_embeddings, [self.dim_head_qk, self.dim_head_qk, self.dim_head_v], dim=0)\n",
    "        qr = torch.einsum('bgci,cij->bgij', q, q_embedding)\n",
    "        kr = torch.einsum('bgci,cij->bgij', k, k_embedding).transpose(2, 3)\n",
    "        qk = torch.einsum('bgci,bgcj->bgij', q, k)\n",
    "\n",
    "        # Multiply by factors\n",
    "        qr = torch.mul(qr, self.f_qr)\n",
    "        kr = torch.mul(kr, self.f_kr)\n",
    "\n",
    "        stacked_similarity = torch.cat([qk, qr, kr], dim=1)\n",
    "        stacked_similarity = self.bn_similarity(stacked_similarity).view(N * W, 3, self.heads, H, H).sum(dim=1)\n",
    "        similarity = torch.softmax(stacked_similarity, dim=3)\n",
    "        sv = torch.einsum('bgij,bgcj->bgci', similarity, v)\n",
    "        sve = torch.einsum('bgij,cij->bgci', similarity, v_embedding)\n",
    "\n",
    "        # Multiply by factors\n",
    "        sv = torch.mul(sv, self.f_sv)\n",
    "        sve = torch.mul(sve, self.f_sve)\n",
    "\n",
    "        stacked_output = torch.cat([sv, sve], dim=-1).view(N * W, self.out_channels * 2, H)\n",
    "        output = self.bn_output(stacked_output).view(N, W, self.out_channels, 2, H).sum(dim=-2)\n",
    "\n",
    "        if self.flag:\n",
    "            output = output.permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            output = output.permute(0, 2, 3, 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.relative, 0., math.sqrt(1. / self.dim_head_v))\n",
    "\n",
    "class AxialLOB(nn.Module):\n",
    "    def __init__(self, W, H, c_in, c_out, c_final, n_heads, pool_kernel, pool_stride, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Channel output of the CNN_in is the channel input for the axial layer\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.c_final = c_final\n",
    "\n",
    "        self.CNN_in = nn.Conv2d(in_channels=1, out_channels=c_in, kernel_size=1)\n",
    "        self.CNN_out = nn.Conv2d(in_channels=c_out, out_channels=c_final, kernel_size=1)\n",
    "        self.CNN_res2 = nn.Conv2d(in_channels=c_out, out_channels=c_final, kernel_size=1)\n",
    "        self.CNN_res1 = nn.Conv2d(in_channels=1, out_channels=c_out, kernel_size=1)\n",
    "\n",
    "        self.norm = nn.BatchNorm2d(c_in)\n",
    "        self.res_norm2 = nn.BatchNorm2d(c_final)\n",
    "        self.res_norm1 = nn.BatchNorm2d(c_out)\n",
    "        self.norm2 = nn.BatchNorm2d(c_final)\n",
    "        self.axial_height_1 = GatedAxialAttention(c_out, c_out, n_heads, H, flag=False)\n",
    "        self.axial_width_1 = GatedAxialAttention(c_out, c_out, n_heads, W, flag=True)\n",
    "        self.axial_height_2 = GatedAxialAttention(c_out, c_out, n_heads, H, flag=False)\n",
    "        self.axial_width_2 = GatedAxialAttention(c_out, c_out, n_heads, W, flag=True)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear = nn.Linear(c_final * (W // pool_stride[1]) * (H // pool_stride[0]), num_classes)\n",
    "        self.pooling = nn.AvgPool2d(kernel_size=pool_kernel, stride=pool_stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Up branch\n",
    "        # First convolution before the attention\n",
    "        y = self.CNN_in(x)\n",
    "        y = self.norm(y)\n",
    "        y = self.activation(y)\n",
    "\n",
    "        # Attention mechanism through gated multi-head axial layers\n",
    "        y = self.axial_width_1(y)\n",
    "        y = self.axial_height_1(y)\n",
    "\n",
    "        # Lower branch\n",
    "        x_res = self.CNN_res1(x)\n",
    "        x_res = self.res_norm1(x_res)\n",
    "        x_res = self.activation(x_res)\n",
    "\n",
    "        # First residual connection\n",
    "        y = y + x_res\n",
    "        z = y.detach().clone()\n",
    "\n",
    "        # Second axial layer\n",
    "        y = self.axial_width_2(y)\n",
    "        y = self.axial_height_2(y)\n",
    "\n",
    "        # Second convolution\n",
    "        y = self.CNN_out(y)\n",
    "        y = self.res_norm2(y)\n",
    "        y = self.activation(y)\n",
    "\n",
    "        # Lower branch\n",
    "        z = self.CNN_res2(z)\n",
    "        z = self.norm2(z)\n",
    "        z = self.activation(z)\n",
    "\n",
    "        # Second residual connection\n",
    "        y = y + z\n",
    "\n",
    "        # Final part\n",
    "        y = self.pooling(y)\n",
    "        y = torch.flatten(y, 1)\n",
    "        y = self.linear(y)\n",
    "        forecast_y = torch.softmax(y, dim=1)\n",
    "        return forecast_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d31f271-db76-4c94-b474-b6226b32a709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BinCTABL                                 [3]                       --\n",
       "├─BiN: 1-1                               [1, 40, 100]              282\n",
       "├─BL_layer: 1-2                          [1, 40, 100]              15,600\n",
       "│    └─ReLU: 2-1                         [1, 40, 100]              --\n",
       "├─Dropout: 1-3                           [1, 40, 100]              --\n",
       "├─BL_layer: 1-4                          [1, 120, 50]              15,800\n",
       "│    └─ReLU: 2-2                         [1, 120, 50]              --\n",
       "├─Dropout: 1-5                           [1, 120, 50]              --\n",
       "├─TABL_layer: 1-6                        [1, 3, 1]                 2,914\n",
       "==========================================================================================\n",
       "Total params: 34,596\n",
       "Trainable params: 34,596\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.03\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 50 \n",
    "c_final = 4              # channel output size of the second conv\n",
    "n_heads = 4\n",
    "c_in_axial = 32          # channel output size of the first conv\n",
    "c_out_axial = 32\n",
    "pool_kernel = (1, 4)\n",
    "pool_stride = (1, 4)\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "W = 40 \n",
    "H = 100 \n",
    "\n",
    "input_dim = (1, H, W)  # [channels, time, features]\n",
    "output_dim = num_classes\n",
    "\n",
    "Model_AxialLOB = AxialLOB(\n",
    "    W=W,\n",
    "    H=H,\n",
    "    c_in=c_in_axial,\n",
    "    c_out=c_out_axial,\n",
    "    c_final=c_final,\n",
    "    n_heads=n_heads,\n",
    "    pool_kernel=pool_kernel,\n",
    "    pool_stride=pool_stride,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "summary(Model_AxialLOB, input_size=(1, 1, H, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "349cdac5-64f6-411e-acff-2ac5d892be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(Model_AxialLOB.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8cc05-cc58-4ec6-acf1-766dcf31c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to encapsulate the training loop\n",
    "def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):\n",
    "    \n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "    best_test_loss = np.inf\n",
    "    best_test_epoch = 0\n",
    "\n",
    "    for it in tqdm(range(epochs)):\n",
    "        \n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        for inputs, targets in train_loader:\n",
    "            # move data to GPU\n",
    "            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "            # print(\"inputs.shape:\", inputs.shape)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            # print(\"about to get model output\")\n",
    "            outputs = model(inputs)\n",
    "            # print(\"done getting model output\")\n",
    "            # print(\"outputs.shape:\", outputs.shape, \"targets.shape:\", targets.shape)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # print(\"about to optimize\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        # Get train loss and test loss\n",
    "        train_loss = np.mean(train_loss) # a little misleading\n",
    "    \n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)      \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss.append(loss.item())\n",
    "        test_loss = np.mean(test_loss)\n",
    "\n",
    "        # Save losses\n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it] = test_loss\n",
    "        \n",
    "        if test_loss < best_test_loss:\n",
    "            torch.save(model, './best_val_model_AxialLOB_pytorch')\n",
    "            best_test_loss = test_loss\n",
    "            best_test_epoch = it\n",
    "            print('model saved')\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "          Validation Loss: {test_loss:.4f}, Duration: {dt}, Best Val Epoch: {best_test_epoch}')\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfd3221-a98b-49b7-b52d-686e50e68bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_AxialLOB, val_losses_AxialLOB = batch_gd(Model_AxialLOB, criterion, optimizer, \n",
    "                                    train_loader, val_loader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbea57a-cb04-47e6-bca4-ecff1322277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Save training losses\n",
    "with open('train_losses_AxialLOB.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Epoch', 'Train_Loss'])\n",
    "    for epoch, loss in enumerate(train_losses_AxialLOB, start=1):\n",
    "        writer.writerow([epoch, loss])\n",
    "\n",
    "# Save validation losses\n",
    "with open('val_losses_AxialLOB.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Epoch', 'Val_Loss'])\n",
    "    for epoch, loss in enumerate(val_losses_AxialLOB, start=1):\n",
    "        writer.writerow([epoch, loss])\n",
    "\n",
    "print(\"Training and validation losses have been saved as CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f32653-5da0-44fb-9d01-ee28d03c396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(train_losses_AxialLOB, label='train loss - AxialLOB')\n",
    "plt.plot(val_losses_AxialLOB, label='validation loss - AxialLOB')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63552c11-21d5-4caf-8236-66d82622bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('best_val_model_AxialLOB_pytorch')\n",
    "\n",
    "n_correct = 0.\n",
    "n_total = 0.\n",
    "for inputs, targets in test_loader:\n",
    "    # Move to GPU\n",
    "    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    # torch.max returns both max and argmax\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "    # update counts\n",
    "    n_correct += (predictions == targets).sum().item()\n",
    "    n_total += targets.shape[0]\n",
    "\n",
    "test_acc = n_correct / n_total\n",
    "print(f\"Test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972fcc4-abe2-421b-8ee0-c4f352565e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('best_val_model_pytorch')\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "for inputs, targets in test_loader:\n",
    "    # Move to GPU\n",
    "    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    # torch.max returns both max and argmax\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "    all_targets.append(targets.cpu().numpy())\n",
    "    all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "all_targets = np.concatenate(all_targets)    \n",
    "all_predictions = np.concatenate(all_predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852311c-f94c-4dc7-8278-7fff17454767",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
